[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The symbol “(x)” in our lab’s name is an experience variable and the testament to our multifaceted approach towards inclusivity and accessibility:\nBy focusing on (x)Ability rather than (dis)Ability, we aim to unlock each person’s unlimited potential. Our lab is driven by the principle of designing with, by, for, and of people with (dis)Abilities. We strive to remove the “(x)”—the barriers—from people with (dis)Abilities, facilitating their access to information systems and contributing to a more inclusive world."
  },
  {
    "objectID": "about.html#our-mission",
    "href": "about.html#our-mission",
    "title": "About",
    "section": "Our Mission",
    "text": "Our Mission\nOur mission is to advance the field of inclusive UX and LX design by: - Conducting groundbreaking research that addresses the challenges faced by people with (dis)Abilities. - Developing tools and methods that enhance the accessibility of digital environments. - Collaborating with communities to ensure our solutions are informed by the real needs and experiences of people with (dis)Abilities.\nJoin us in redefining accessibility and inclusivity at the (x)Ability Design Lab. Because everyone deserves to experience the world without barriers."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Researchers at NewHaptics Corporation and the University of Illinois at Urbana-Champaign are developing and testing software and hardware tools for faculty, staff, postdoctoral research fellows, and graduate students who are blind or have low vision, and who are working in college and university Chemistry, Mathematics, and Computer Science and Engineering and Information Sciences employment settings. These tools, which use a combination of hearing and touch technologies, are making it possible for people who are blind or have low vision to generate, identify, and manipulate digital data patterns and trends.\nGiven the scarcity of STEM scientists, researchers, and educators in our country, opportunities to increase knowledge about better data use technologies is essential to the retention and advancement of students and professionals who are blind or have low vision.\n\n\n\nDevelop new hardware interaction components and firmware/drivers;\nDevelop new interaction software, and\nEvaluate the system and conduct workshops for users.\n\nThe project activities are addressing the following key research questions: 1) How can the addition of spatial information, realized through multiline braille and large array tactile graphics hardware, coupled with interactive software tools, break down barriers to data use? 2) What design considerations contribute to multi-modal data, that is, verbal, sonification, and tactile data representations that go beyond a single perceptual modality? Answers to these questions have the potential to contribute to research about STEM postsecondary workplace solutions for people with disabilities."
  },
  {
    "objectID": "projects.html#an-audiotactile-data-system-for-blind-or-low-vision-faculty-staff-postdocs-and-graduate-students-in-chemistry-math-computer-and-information-sciences",
    "href": "projects.html#an-audiotactile-data-system-for-blind-or-low-vision-faculty-staff-postdocs-and-graduate-students-in-chemistry-math-computer-and-information-sciences",
    "title": "Projects",
    "section": "",
    "text": "Researchers at NewHaptics Corporation and the University of Illinois at Urbana-Champaign are developing and testing software and hardware tools for faculty, staff, postdoctoral research fellows, and graduate students who are blind or have low vision, and who are working in college and university Chemistry, Mathematics, and Computer Science and Engineering and Information Sciences employment settings. These tools, which use a combination of hearing and touch technologies, are making it possible for people who are blind or have low vision to generate, identify, and manipulate digital data patterns and trends.\nGiven the scarcity of STEM scientists, researchers, and educators in our country, opportunities to increase knowledge about better data use technologies is essential to the retention and advancement of students and professionals who are blind or have low vision.\n\n\n\nDevelop new hardware interaction components and firmware/drivers;\nDevelop new interaction software, and\nEvaluate the system and conduct workshops for users.\n\nThe project activities are addressing the following key research questions: 1) How can the addition of spatial information, realized through multiline braille and large array tactile graphics hardware, coupled with interactive software tools, break down barriers to data use? 2) What design considerations contribute to multi-modal data, that is, verbal, sonification, and tactile data representations that go beyond a single perceptual modality? Answers to these questions have the potential to contribute to research about STEM postsecondary workplace solutions for people with disabilities."
  },
  {
    "objectID": "projects.html#maidr-multimodal-access-and-interactive-data-representation",
    "href": "projects.html#maidr-multimodal-access-and-interactive-data-representation",
    "title": "Projects",
    "section": "MAIDR: Multimodal Access and Interactive Data Representation",
    "text": "MAIDR: Multimodal Access and Interactive Data Representation\nAssistant Professor JooYoung Seo has been awarded a $649,921 Early Career Development grant from the Institute of Museum and Library Services, under the Laura Bush 21st Century Librarian Program, which supports “developing a diverse workforce of librarians to better meet the changing learning and information needs of the American public by enhancing the training and professional development of librarians, developing faculty and library leaders, and recruiting and educating the next generation of librarians.”\nThe three-year grant is an extension of Seo’s ongoing project, “MAIDR: Multimodal Access and Interactive Data Representation,” that has received support from the International Society of the Learning Sciences and the Wallace Foundation. Through the initial work, Seo has been developing computer tools that augment visual charts with touchable (braille), readable (text), and audible (sound) representations, to make them more accessible for the visually impaired. In the new project, Seo’s tools will connect the multimodal and accessible data representation with data curators’ day-to-day reproducible workflows, integrating them into reproducible frameworks (e.g., Jupyter Notebook, R Markdown, Quarto) and visualization libraries (e.g., R ggplot2, Python matplotlib).\n“Multimodal data representation is not merely an innovation; it is an imperative for inclusivity,” Seo said. “While data visualization is a dominant method in scientific representation, it inherently marginalizes those who are blind and visually impaired, essentially sidelining them from full participation in the scientific discourse. My multimodal data representation challenges this norm by expanding the toolkit beyond the visual. It incorporates auditory, tactile, and verbal methods, thereby democratizing access to knowledge.”\nAccording to Seo, this approach enriches the understanding of data for everyone and offers multiple perspectives that a singular approach could easily miss.\nThe new project will involve meeting with data curators to assess needs and with blind patrons to shape the tools being created. For that work, Seo will collaborate with his community partners, Data Curation Network (DCN) and the National Federation of the Blind (NFB).\nPartners on the project include the National Center for Supercomputing Applications (NCSA), Posit Public Benefit Corporation (FKA, RStudio), and the Chart2Music (C2M) open-source project team.\n\nIMLS grant RE-254891-OLS-23"
  },
  {
    "objectID": "projects.html#promoting-computational-thinking-skills-for-blind-and-visually-impaired-teens-through-accessible-library-makerspaces",
    "href": "projects.html#promoting-computational-thinking-skills-for-blind-and-visually-impaired-teens-through-accessible-library-makerspaces",
    "title": "Projects",
    "section": "Promoting Computational Thinking Skills for Blind and Visually Impaired Teens Through Accessible Library Makerspaces",
    "text": "Promoting Computational Thinking Skills for Blind and Visually Impaired Teens Through Accessible Library Makerspaces\nLibrary makerspaces offer community members the opportunity to tinker, design, experiment, and create with a range of technology in an informal learning space. However, because current makerspaces and maker tools are highly vision oriented, blind and visually impaired (BVI) people have limited access to these learning opportunities. A new project led by Assistant Professor JooYoung Seo and Associate Professor Kyungwon Koh, director of the CU Community Fab Lab, seeks to address this problem by creating accessible maker programs for BVI learners and developing training materials on accessible making for librarians and maker professionals. The researchers were recently awarded a three-year, $498,638 National Leadership Grant from the Institute of Museum and Library Services for their project, “Promoting Computational Thinking Skills for Blind and Visually Impaired Teens Through Accessible Library Makerspaces.”\nFor the project, the iSchool and CU Community Fab Lab will partner with the American Printing House for the Blind, Young Adult Library Services Association, and Reaching Across Illinois Library System Makerspace Networking Group. The research also has received support from the National Federation of the Blind (NFB) and the Disability Resources and Educational Services (DRES) and Information Accessibility Design and Policy (IADP) program at the University of Illinois. Activities will include training maker professionals and conducting an accessibility status assessment, hosting a summer camp with BVI teens to co-design accessible maker curriculum, testing the developed accessible maker programs in four Illinois library makerspaces, and training library users who will benefit from a more inclusive and accessible makerspace.\n“Just as curb cuts help more than a person who uses a wheelchair, accessibility features added to maker tools and learning materials can make the system more usable by everyone,” said Seo. “The tangible making activities and integrated curricula in our project will bring the current maker movement a new insight into how we can broaden the participation of maker and STEM learning for underserved populations of diverse abilities.”\nThe goal of the makerspace project is to not only enhance BVI learners’ computational thinking skills and STEM interests but also help librarians and maker professionals become more confident and capable when working with BVI populations.\n\nIMLS grant LG-252360-OLS-22"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "The following includes the recent publications in our lab (including outcomes with other collaborators).\n\n\n1. Ge, K., & Seo, J. (2024, October 28). StereoMath: An Accessible and Musical Equation Editor. Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility. https://doi.org/10.1145/3663548.3688487\n\n\n2. Kamath, S. S., Zeidieh, A., Khan, O., Sethi, D., & Seo, J. (2024, October 28). Playing Without Barriers: Crafting Playful and Accessible VR Table-Tennis with and for Blind and Low-Vision Individuals. Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility. https://doi.org/10.1145/3663548.3675660\n\n\n3. Seo, J., Kamath, S. S., Zeidieh, A., Venkatesh, S., & McCurry, S. (2024, October 28). MAIDR Meets AI: Exploring Multimodal LLM-Based Data Visualization Interpretation by and with Blind and Low-Vision Users. Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility. https://doi.org/10.1145/3663548.3675660\n\n\n4. Zhou, K. Z., Adler, R. F., Almendral, C., Choi, S., Kletenik, D., Oro, B., & Seo, J. (2024, October 28). Teaching Accessibility in Different Disciplines: Topics, Approaches, Resources, Challenges. Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility. https://doi.org/10.1145/3663548.3688553\n\n\n5. Chen, S., Cox, E., Koh, K., & Seo, J. (2024). The role of verbal descriptions in facilitating computational thinking skills for BVI learners. In R. Lindgren, T. I. Asino, E. A. Kyza, C. K. Looi, D. T. Keifert, & E. Suárez (Eds.), Proceedings of the 18th international conference of the learning sciences - ICLS 2024 (pp. 2343–2344). International Society of the Learning Sciences. https://doi.org/10.22318/icls2024.594746\n\n\n6. Jung, Y. J., Chang, Y., Moon, J., Seo, J., Bonnette, R., Lee, J.-Y., Ke, F., Sokolikj, Z., Koh, K., Cox, E., Chen, S., Abbas, J., Munyao, M., DiCioccio, M., & Alstad, Z. (2024). Learning environments designed for and with learners with disabilities. In R. Lindgren, T. I. Asino, E. A. Kyza, C. K. Looi, D. T. Keifert, & E. Suárez (Eds.), Proceedings of the 18th international conference of the learning sciences - ICLS 2024 (pp. 1918–1925). International Society of the Learning Sciences. https://doi.org/10.22318/icls2024.384050\n\n\n7. Lee, J. G. W., Lee, B., Choi, S., Seo, J., & Choe, E. K. (2024). Identify, Adapt, Persist: The Journey of Blind Individuals with Personal Health Technologies. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 8(2), 51:1–51:21. https://doi.org/10.1145/3659585\n\n\n8. Seo, J., O’Modhrain, S., Xia, Y., Kamath, S., Lee, B., & Coughlan, J. M. (2024). Designing Born-Accessible Courses in Data Science and Visualization: Challenges and Opportunities of a Remote Curriculum Taught by Blind Instructors to Blind Students. In R. S. and A. Firat Elif E. and Laramee (Ed.), EuroVis 2024 - education papers. The Eurographics Association. https://doi.org/10.2312/eved.20241053\n\n\n9. Seo, J., Xia, Y., Lee, B., Mccurry, S., & Yam, Y. J. (2024). MAIDR: Making Statistical Visualizations Accessible with Multimodal Data Representation. Proceedings of the CHI Conference on Human Factors in Computing Systems, 1–22. https://doi.org/10.1145/3613904.3642730\n\n\n10. Kaushik, S., Barbosa, N. M., Yu, Y., Sharma, T., Kilhoffer, Z., Seo, J., Das, S., & Wang, Y. (2023). GuardLens: Supporting safer online browsing for people with visual impairments. Nineteenth Symposium on Usable Privacy and Security (SOUPS 2023), 361–380. https://www.usenix.org/conference/soups2023/presentation/kaushik\n\n\n11. Kim, S. H., Yoon, A., & Seo, J. (2023). Trend of Collaboration in STEM Education in Informal Learning Institutions Based on IMLS-funded Projects. Proceedings of the Association for Information Science and Technology, 60(1), 625–629. https://doi.org/10.1002/pra2.828\n\n\n12. Park, J., Seo, J., & Lee, J. Y. (2023). Exploring an Online Community of Blind Programmers by Using Topic Modeling and Network Analysis. Proceedings of the Association for Information Science and Technology, 60(1), 1096–1098. https://doi.org/10.1002/pra2.956\n\n\n13. Seo, J., & Rogge, M. (2023). Coding Non-Visually in Visual Studio Code: Collaboration Towards Accessible Development Environment for Blind Programmers. Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility, 1–9. https://doi.org/10.1145/3597638.3614550\n\n\n14. Seo, J., Xia, Y., Yam, Y. J., & McCurry, S. (2023). MAIDR: Multimodal Access and Interactive Data Representation System for Inclusive Data Science Education. Proceedings of the 3rd Annual Meeting of the International Society of the Learning Sciences, 51–54.\n\n\n15. Zhang, Z. (Jerry)., Kaushik, S., Seo, J., Yuan, H., Das, S., Findlater, L., Gurari, D., Stangl, A., & Wang, Y. (2023). ImageAlly: A Human-AI hybrid approach to support blind people in detecting and redacting private image content. Nineteenth Symposium on Usable Privacy and Security (SOUPS 2023), 417–436. https://www.usenix.org/conference/soups2023/presentation/zhang\n\n\n16. Huh, M., & Seo, J. (2022). A duoethnographic study of a mixed-ability team in a collaborative group programming project. In A. Weinberger, W. Chen, D. Hernández-Leo, & B. Chen (Eds.), Proceedings of the 15th international conference on computer-supported collaborative learning - CSCL 2022 (pp. 471–474). International Society of the Learning Sciences.\n\n\n17. Lee, C. Y. P., Zhang, Z., Herskovitz, J., Seo, J., & Guo, A. (2022). CollabAlly: Accessible Collaboration Awareness in Document Editing. CHI Conference on Human Factors in Computing Systems, 1–17. https://doi.org/10.1145/3491102.3517635. Honorable mention award.\n\n\n18. Seo, J., & Dogucu, M. (2022). Teaching visual accessibility in the introductory data science classes: Why, what, when, and how. The 2022 Symposium on Data Science & Statistics.\n\n\n19. Lee, C. Y. P., Zhang, Z., Herskovitz, J., Seo, J., & Guo, A. (2021). CollabAlly: Accessible collaboration awareness in document editing. The 23rd International ACM SIGACCESS Conference on Computers and Accessibility, 1–4. https://doi.org/10.1145/3441852.3476562"
  },
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "People",
    "section": "",
    "text": "Dr. JooYoung Seo leads the (x)Ability Design Lab, where their research focuses on accessibility research. He is an assistant professor in the School of Information Sciences and a faculty affiliate in the Department of Computer Science, Informatics Institute, and the National Center for Supercomputing Applications at the University of Illinois Urbana-Champaign. Seo is also an RStudio double-certified data science instructor and accessibility expert certified by the International Association of Accessibility Professionals (IAAP). His research topics involve accessible computing, universal design, inclusive data science, and equitable healthcare technologies. As an emerging learning scientist and information scientist, his research focuses particularly on how to make computational literacy more accessible to people with dis/abilities by using multimodal data representation.\nHe has worked on various research and development projects on accessible computing and open-source data science packages (e.g., gt; shiny; rmarkdown to name a few) for accessibility. His research projects have involved not just web accessibility, but also human-centered design and development studies, including inclusive makerspaces, tangible block-based programming, accessible data science (e.g., data tactualization, sonification, and verbalization), and accessible/reproducible scientific writing tools for people with and without dis/abilities.\nHis research is funded by national institutes, industrial partners, and academic society, such as Institute of Museum and Library Services (IMLS), National Science Foundation (NSF), National Institute on Disability, Independent Living, and Rehabilitation Research (NIDILRR), Posit Software, PBC (formerly RStudio), Teach Access, and the International Society of the Learning Sciences (ISLS) and the Wallace Foundation.\nSeo obtained his PhD and M.Ed in at the Pennsylvania State University, and double BA in education and English literature from Sungkyunkwan University, Seoul, South Korea."
  },
  {
    "objectID": "people.html#principal-investigator",
    "href": "people.html#principal-investigator",
    "title": "People",
    "section": "",
    "text": "Dr. JooYoung Seo leads the (x)Ability Design Lab, where their research focuses on accessibility research. He is an assistant professor in the School of Information Sciences and a faculty affiliate in the Department of Computer Science, Informatics Institute, and the National Center for Supercomputing Applications at the University of Illinois Urbana-Champaign. Seo is also an RStudio double-certified data science instructor and accessibility expert certified by the International Association of Accessibility Professionals (IAAP). His research topics involve accessible computing, universal design, inclusive data science, and equitable healthcare technologies. As an emerging learning scientist and information scientist, his research focuses particularly on how to make computational literacy more accessible to people with dis/abilities by using multimodal data representation.\nHe has worked on various research and development projects on accessible computing and open-source data science packages (e.g., gt; shiny; rmarkdown to name a few) for accessibility. His research projects have involved not just web accessibility, but also human-centered design and development studies, including inclusive makerspaces, tangible block-based programming, accessible data science (e.g., data tactualization, sonification, and verbalization), and accessible/reproducible scientific writing tools for people with and without dis/abilities.\nHis research is funded by national institutes, industrial partners, and academic society, such as Institute of Museum and Library Services (IMLS), National Science Foundation (NSF), National Institute on Disability, Independent Living, and Rehabilitation Research (NIDILRR), Posit Software, PBC (formerly RStudio), Teach Access, and the International Society of the Learning Sciences (ISLS) and the Wallace Foundation.\nSeo obtained his PhD and M.Ed in at the Pennsylvania State University, and double BA in education and English literature from Sungkyunkwan University, Seoul, South Korea."
  },
  {
    "objectID": "people.html#ph.d.-students",
    "href": "people.html#ph.d.-students",
    "title": "People",
    "section": "Ph.D. Students",
    "text": "Ph.D. Students\n\n\n\n\n\n\nSanchita Kamath\n\n\nPh.D. Student, Information Sciences\n\n\n\n\nHer research focuses on designing and developing immersive, user-centered solutions that enhance accessibility and inclusivity, particularly for blind and low-vision (BLV) individuals. With a background in Computer Science and Engineering, Sanchita’s work seeks to bridge the gap between cutting-edge technology and user experience. She is currently workign on studies that investigate cognition in BLV people and multiple tools and technologies through which this can be achieved.\n\n\n\n\n\n\n\n\n\nAziz N. Zeidieh\n\n\nPh.D. Student, Informatics\n\n\n\n\nAziz is a data-driven human-computer interaction researcher working at the intersection of orientation and mobility, artificial intelligence, and assistive technology. His current work focuses on utilizing multimodal AI models to provide an accessible and interactive application for the spatial orientation of blind and visually impaired (BVI) travelers. He is actively exploring opportunities to leverage data and the lived experiences of BVI individuals to facilitate safer and more efficient technologically interdependent travel experiences."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "(x)Ability Design Lab",
    "section": "",
    "text": "Welcome to the (x)Ability Design Lab, pronounced as “Accessibility Design” Lab, where the “x” embodies our mission to enhance user experience (UX) and learning experience (LX) by dismantling sociotechnical barriers associated with disability. At our core, we believe in focusing on the ability and experience of all individuals, moving beyond the constraints of traditional accessibility discourse."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact Us",
    "section": "",
    "text": "If you would like to discuss a project or have any questions, please feel free to email us at xability-lab@illinois.edu."
  }
]