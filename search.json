[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Researchers at NewHaptics Corporation and the University of Illinois at Urbana-Champaign are developing and testing software and hardware tools for faculty, staff, postdoctoral research fellows, and graduate students who are blind or have low vision, and who are working in college and university Chemistry, Mathematics, and Computer Science and Engineering and Information Sciences employment settings. These tools, which use a combination of hearing and touch technologies, are making it possible for people who are blind or have low vision to generate, identify, and manipulate digital data patterns and trends.\nGiven the scarcity of STEM scientists, researchers, and educators in our country, opportunities to increase knowledge about better data use technologies is essential to the retention and advancement of students and professionals who are blind or have low vision.\n\n\n\nDevelop new hardware interaction components and firmware/drivers;\nDevelop new interaction software, and\nEvaluate the system and conduct workshops for users.\n\nThe project activities are addressing the following key research questions: 1) How can the addition of spatial information, realized through multiline braille and large array tactile graphics hardware, coupled with interactive software tools, break down barriers to data use? 2) What design considerations contribute to multi-modal data, that is, verbal, sonification, and tactile data representations that go beyond a single perceptual modality? Answers to these questions have the potential to contribute to research about STEM postsecondary workplace solutions for people with disabilities."
  },
  {
    "objectID": "projects.html#an-audiotactile-data-system-for-blind-or-low-vision-faculty-staff-postdocs-and-graduate-students-in-chemistry-math-computer-and-information-sciences",
    "href": "projects.html#an-audiotactile-data-system-for-blind-or-low-vision-faculty-staff-postdocs-and-graduate-students-in-chemistry-math-computer-and-information-sciences",
    "title": "Projects",
    "section": "",
    "text": "Researchers at NewHaptics Corporation and the University of Illinois at Urbana-Champaign are developing and testing software and hardware tools for faculty, staff, postdoctoral research fellows, and graduate students who are blind or have low vision, and who are working in college and university Chemistry, Mathematics, and Computer Science and Engineering and Information Sciences employment settings. These tools, which use a combination of hearing and touch technologies, are making it possible for people who are blind or have low vision to generate, identify, and manipulate digital data patterns and trends.\nGiven the scarcity of STEM scientists, researchers, and educators in our country, opportunities to increase knowledge about better data use technologies is essential to the retention and advancement of students and professionals who are blind or have low vision.\n\n\n\nDevelop new hardware interaction components and firmware/drivers;\nDevelop new interaction software, and\nEvaluate the system and conduct workshops for users.\n\nThe project activities are addressing the following key research questions: 1) How can the addition of spatial information, realized through multiline braille and large array tactile graphics hardware, coupled with interactive software tools, break down barriers to data use? 2) What design considerations contribute to multi-modal data, that is, verbal, sonification, and tactile data representations that go beyond a single perceptual modality? Answers to these questions have the potential to contribute to research about STEM postsecondary workplace solutions for people with disabilities."
  },
  {
    "objectID": "projects.html#maidr-multimodal-access-and-interactive-data-representation",
    "href": "projects.html#maidr-multimodal-access-and-interactive-data-representation",
    "title": "Projects",
    "section": "MAIDR: Multimodal Access and Interactive Data Representation",
    "text": "MAIDR: Multimodal Access and Interactive Data Representation\nAssistant Professor JooYoung Seo has been awarded a $649,921 Early Career Development grant from the Institute of Museum and Library Services, under the Laura Bush 21st Century Librarian Program, which supports “developing a diverse workforce of librarians to better meet the changing learning and information needs of the American public by enhancing the training and professional development of librarians, developing faculty and library leaders, and recruiting and educating the next generation of librarians.”\nThe three-year grant is an extension of Seo’s ongoing project, “MAIDR: Multimodal Access and Interactive Data Representation,” that has received support from the International Society of the Learning Sciences and the Wallace Foundation. Through the initial work, Seo has been developing computer tools that augment visual charts with touchable (braille), readable (text), and audible (sound) representations, to make them more accessible for the visually impaired. In the new project, Seo’s tools will connect the multimodal and accessible data representation with data curators’ day-to-day reproducible workflows, integrating them into reproducible frameworks (e.g., Jupyter Notebook, R Markdown, Quarto) and visualization libraries (e.g., R ggplot2, Python matplotlib).\n“Multimodal data representation is not merely an innovation; it is an imperative for inclusivity,” Seo said. “While data visualization is a dominant method in scientific representation, it inherently marginalizes those who are blind and visually impaired, essentially sidelining them from full participation in the scientific discourse. My multimodal data representation challenges this norm by expanding the toolkit beyond the visual. It incorporates auditory, tactile, and verbal methods, thereby democratizing access to knowledge.”\nAccording to Seo, this approach enriches the understanding of data for everyone and offers multiple perspectives that a singular approach could easily miss.\nThe new project will involve meeting with data curators to assess needs and with blind patrons to shape the tools being created. For that work, Seo will collaborate with his community partners, Data Curation Network (DCN) and the National Federation of the Blind (NFB).\nPartners on the project include the National Center for Supercomputing Applications (NCSA), Posit Public Benefit Corporation (FKA, RStudio), and the Chart2Music (C2M) open-source project team.\n\nIMLS grant RE-254891-OLS-23"
  },
  {
    "objectID": "projects.html#promoting-computational-thinking-skills-for-blind-and-visually-impaired-teens-through-accessible-library-makerspaces",
    "href": "projects.html#promoting-computational-thinking-skills-for-blind-and-visually-impaired-teens-through-accessible-library-makerspaces",
    "title": "Projects",
    "section": "Promoting Computational Thinking Skills for Blind and Visually Impaired Teens Through Accessible Library Makerspaces",
    "text": "Promoting Computational Thinking Skills for Blind and Visually Impaired Teens Through Accessible Library Makerspaces\nLibrary makerspaces offer community members the opportunity to tinker, design, experiment, and create with a range of technology in an informal learning space. However, because current makerspaces and maker tools are highly vision oriented, blind and visually impaired (BVI) people have limited access to these learning opportunities. A new project led by Assistant Professor JooYoung Seo and Associate Professor Kyungwon Koh, director of the CU Community Fab Lab, seeks to address this problem by creating accessible maker programs for BVI learners and developing training materials on accessible making for librarians and maker professionals. The researchers were recently awarded a three-year, $498,638 National Leadership Grant from the Institute of Museum and Library Services for their project, “Promoting Computational Thinking Skills for Blind and Visually Impaired Teens Through Accessible Library Makerspaces.”\nFor the project, the iSchool and CU Community Fab Lab will partner with the American Printing House for the Blind, Young Adult Library Services Association, and Reaching Across Illinois Library System Makerspace Networking Group. The research also has received support from the National Federation of the Blind (NFB) and the Disability Resources and Educational Services (DRES) and Information Accessibility Design and Policy (IADP) program at the University of Illinois. Activities will include training maker professionals and conducting an accessibility status assessment, hosting a summer camp with BVI teens to co-design accessible maker curriculum, testing the developed accessible maker programs in four Illinois library makerspaces, and training library users who will benefit from a more inclusive and accessible makerspace.\n“Just as curb cuts help more than a person who uses a wheelchair, accessibility features added to maker tools and learning materials can make the system more usable by everyone,” said Seo. “The tangible making activities and integrated curricula in our project will bring the current maker movement a new insight into how we can broaden the participation of maker and STEM learning for underserved populations of diverse abilities.”\nThe goal of the makerspace project is to not only enhance BVI learners’ computational thinking skills and STEM interests but also help librarians and maker professionals become more confident and capable when working with BVI populations.\n\nIMLS grant LG-252360-OLS-22"
  },
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "People",
    "section": "",
    "text": "Dr. JooYoung Seo leads the (x)Ability Design Lab, where their research focuses on accessibility research. He is an assistant professor in the School of Information Sciences and a faculty affiliate in the Department of Computer Science, Informatics Institute, and the National Center for Supercomputing Applications at the University of Illinois Urbana-Champaign. Seo is also an RStudio double-certified data science instructor and accessibility expert certified by the International Association of Accessibility Professionals (IAAP). His research topics involve accessible computing, universal design, inclusive data science, and equitable healthcare technologies. As an emerging learning scientist and information scientist, his research focuses particularly on how to make computational literacy more accessible to people with dis/abilities by using multimodal data representation.\nHe has worked on various research and development projects on accessible computing and open-source data science packages (e.g., gt; shiny; rmarkdown to name a few) for accessibility. His research projects have involved not just web accessibility, but also human-centered design and development studies, including inclusive makerspaces, tangible block-based programming, accessible data science (e.g., data tactualization, sonification, and verbalization), and accessible/reproducible scientific writing tools for people with and without dis/abilities.\nHis research is funded by national institutes, industrial partners, and academic society, such as Institute of Museum and Library Services (IMLS), National Science Foundation (NSF), National Institute on Disability, Independent Living, and Rehabilitation Research (NIDILRR), Posit Software, PBC (formerly RStudio), Teach Access, and the International Society of the Learning Sciences (ISLS) and the Wallace Foundation.\nSeo obtained his PhD and M.Ed in at the Pennsylvania State University, and double BA in education and English literature from Sungkyunkwan University, Seoul, South Korea."
  },
  {
    "objectID": "people.html#director",
    "href": "people.html#director",
    "title": "People",
    "section": "",
    "text": "Dr. JooYoung Seo leads the (x)Ability Design Lab, where their research focuses on accessibility research. He is an assistant professor in the School of Information Sciences and a faculty affiliate in the Department of Computer Science, Informatics Institute, and the National Center for Supercomputing Applications at the University of Illinois Urbana-Champaign. Seo is also an RStudio double-certified data science instructor and accessibility expert certified by the International Association of Accessibility Professionals (IAAP). His research topics involve accessible computing, universal design, inclusive data science, and equitable healthcare technologies. As an emerging learning scientist and information scientist, his research focuses particularly on how to make computational literacy more accessible to people with dis/abilities by using multimodal data representation.\nHe has worked on various research and development projects on accessible computing and open-source data science packages (e.g., gt; shiny; rmarkdown to name a few) for accessibility. His research projects have involved not just web accessibility, but also human-centered design and development studies, including inclusive makerspaces, tangible block-based programming, accessible data science (e.g., data tactualization, sonification, and verbalization), and accessible/reproducible scientific writing tools for people with and without dis/abilities.\nHis research is funded by national institutes, industrial partners, and academic society, such as Institute of Museum and Library Services (IMLS), National Science Foundation (NSF), National Institute on Disability, Independent Living, and Rehabilitation Research (NIDILRR), Posit Software, PBC (formerly RStudio), Teach Access, and the International Society of the Learning Sciences (ISLS) and the Wallace Foundation.\nSeo obtained his PhD and M.Ed in at the Pennsylvania State University, and double BA in education and English literature from Sungkyunkwan University, Seoul, South Korea."
  },
  {
    "objectID": "people.html#ph.d.-students",
    "href": "people.html#ph.d.-students",
    "title": "People",
    "section": "Ph.D. Students",
    "text": "Ph.D. Students\n\n\n\n\n\n\nSanchita Kamath\n\n\nPh.D. Candidate, Information Sciences\n\n\n\n\nSanchita Kamath is a Ph.D. candidate in Information Sciences at the University of Illinois Urbana-Champaign, specializing in Human-Computer Interaction, Data Visualization Education, and Embodied Learning. Her research explores how generative AI and immersive technologies can make data experiences more inclusive, non-visual, tangible, and human-centered. At the (x)Ability Lab, Sanchita designs and evaluates multimodal and embodied interfaces that bridge cognitive and physical engagement in data understanding. Her projects include VR-based learning environments, AI-driven visualization tools, and interactive pedagogical frameworks that reimagine how data can be felt, explored, and understood beyond sight. Her broader goal is to advance accessible data education by merging design, technology, and embodied experience to create equitable pathways for learning and exploration.\n\n\n\n\n\n\n\n\n\nAziz N. Zeidieh\n\n\nPh.D. Candidate, Informatics\n\n\n\n\nAziz is a data-driven human-computer interaction researcher working at the intersection of orientation and mobility, artificial intelligence, and assistive technology. His current work focuses on utilizing multimodal AI models to provide an accessible and interactive application for the spatial orientation of blind and visually impaired (BVI) travelers. He is actively exploring opportunities to leverage data and the lived experiences of BVI individuals to facilitate safer and more efficient technologically interdependent travel experiences.\n\n\n\n\n\n\n\n\n\nOmar Khan\n\n\nPh.D. Candidate, Computer Science\n\n\n\n\nOmar is a human-computer interaction researcher working at the intersection of accessibility, human-AI interaction, and inclusive STEM education. His current research focuses on co-designing accessible data visualizations, especially qualitative or ‘open-ended’ data such as concept maps, network graphs, and coding trees, with the blind and low-vision (BLV) community. Through understanding BLV researchers’ workflows and processes, he aims to create responsible, AI-driven digital experiences that empower people of diverse abilities to independently create, interpret, and share complex information with their peers and broader audiences.\n\n\n\n\n\n\n\n\n\nWeijun Zhang\n\n\nPh.D. Student, Information Science\n\n\n\n\nWeijun Zhang is a researcher focusing on accessibility, human-computer interaction (HCI), and universal design in both educational and workplace settings. Blind since birth, he combines lived experience with academic inquiry to explore how emerging technologies can support inclusive design and improve digital accessibility for people with disabilities. Before joining the University of Illinois Urbana-Champaign, Weijun earned a master’s degree in Translation and Interpreting from Beijing International Studies University and another in Cultural Foundations of Education from Syracuse University. In addition to his academic work, he has been actively involved in disability rights advocacy in China for many years. His interdisciplinary background and advocacy experience inform his current research on accessible computing, inclusion in education and Employment, and the development of digital tools that empower people with visual impairments."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "The following includes the recent publications in our lab (including outcomes with other collaborators).\n\n\n1. Khan, O., & Seo, J. (2025). \"Sighted People Have Their Pick Of The Litter\": Unpacking The Need For Digital Mental Health (DMH) Tracking Services With And For The Blind Community. Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, 1–13. https://doi.org/10.1145/3706599.3719817\n\n\n2. Koh, K., Seo, J., Chen, S., & Cox, E. M. (2025). Engaging with information beyond vision: Hands-on approaches to computational thinking for blind and visually impaired learners. Information Research an International Electronic Journal, 30(iConf, iConf), 280–286. https://doi.org/10.47989/ir30iConf47353\n\n\n3. Ge, K., & Seo, J. (2024). StereoMath: An Accessible and Musical Equation Editor. Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility, 1–5. https://doi.org/10.1145/3663548.3688487\n\n\n4. Kamath, S. S., Zeidieh, A., Khan, O., Sethi, D., & Seo, J. (2024). Playing Without Barriers: Crafting Playful and Accessible VR Table-Tennis with and for Blind and Low-Vision Individuals. Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility, 1–5. https://doi.org/10.1145/3663548.3688526\n\n\n5. Seo, J. Y., O’Modhrain, S., Xia, Y., Kamath, S. S., Lee, B., & Coughlan, J. (2024). Designing Born-Accessible Courses in Data Science and Visualization: Challenges and Opportunities of a Remote Curriculum Taught by Blind Instructors to Blind Students. EuroVis 2024 - Education Papers. https://doi.org/10.2312/EVED.20241053\n\n\n6. Seo, J., Kamath, S. S., Zeidieh, A., Venkatesh, S., & McCurry, S. (2024). MAIDR Meets AI: Exploring Multimodal LLM-Based Data Visualization Interpretation by and with Blind and Low-Vision Users. Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility, 1–31. https://doi.org/10.1145/3663548.3675660\n\n\n7. Seo, J., Xia, Y., Lee, B., Mccurry, S., & Yam, Y. J. (2024). MAIDR: Making Statistical Visualizations Accessible with Multimodal Data Representation. Proceedings of the CHI Conference on Human Factors in Computing Systems, 1–22. https://doi.org/10.1145/3613904.3642730\n\n\n8. Seo, J., & Rogge, M. (2023). Coding Non-Visually in Visual Studio Code: Collaboration Towards Accessible Development Environment for Blind Programmers. Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility, 1–9. https://doi.org/10.1145/3597638.3614550"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "(x)Ability Design Lab",
    "section": "",
    "text": "Welcome to the (x)Ability Design Lab, pronounced as “Accessibility Design” Lab, where the “x” embodies our mission to enhance user experience (UX) and learning experience (LX) by dismantling sociotechnical barriers associated with disability. At our core, we believe in focusing on the ability and experience of all individuals, moving beyond the constraints of traditional accessibility discourse."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact Us",
    "section": "",
    "text": "If you would like to discuss a project or have any questions, please feel free to email us at xability-lab@illinois.edu."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The symbol “(x)” in our lab’s name is an experience variable and the testament to our multifaceted approach towards inclusivity and accessibility:\nBy focusing on (x)Ability rather than (dis)Ability, we aim to unlock each person’s unlimited potential. Our lab is driven by the principle of designing with, by, for, and of people with (dis)Abilities. We strive to remove the “(x)”—the barriers—from people with (dis)Abilities, facilitating their access to information systems and contributing to a more inclusive world."
  },
  {
    "objectID": "about.html#our-mission",
    "href": "about.html#our-mission",
    "title": "About",
    "section": "Our Mission",
    "text": "Our Mission\nOur mission is to advance the field of inclusive UX and LX design by: - Conducting groundbreaking research that addresses the challenges faced by people with (dis)Abilities. - Developing tools and methods that enhance the accessibility of digital environments. - Collaborating with communities to ensure our solutions are informed by the real needs and experiences of people with (dis)Abilities.\nJoin us in redefining accessibility and inclusivity at the (x)Ability Design Lab. Because everyone deserves to experience the world without barriers."
  },
  {
    "objectID": "braille-display.html",
    "href": "braille-display.html",
    "title": "How to Request an NLS eReader Braille Display?",
    "section": "",
    "text": "Some of the studies we conduct here at the (x)Ability Design Lab have an eligibility criterion requiring perspective participants to have access to a refreshable braille display. While companies like Orbit Research offer these devices at a relatively lower cost when compared with what’s on the market, we understand that a no-cost solution may be more ideal depending on an individual’s current situation. This is why we have put together this guide to walk you through the process to request and receive a refreshable braille display from the National Library Service for the Blind and Print Disabled (NLS).\nSome important things to note: * You must be an NLS patron. * Availability of the refreshable braille display eReaders from the NLS is not guaranteed. * While there is no cost associated, patrons are loaning these eReaders from the NLS for an unspecified period of time."
  },
  {
    "objectID": "braille-display.html#purpose-of-this-page",
    "href": "braille-display.html#purpose-of-this-page",
    "title": "How to Request an NLS eReader Braille Display?",
    "section": "",
    "text": "Some of the studies we conduct here at the (x)Ability Design Lab have an eligibility criterion requiring perspective participants to have access to a refreshable braille display. While companies like Orbit Research offer these devices at a relatively lower cost when compared with what’s on the market, we understand that a no-cost solution may be more ideal depending on an individual’s current situation. This is why we have put together this guide to walk you through the process to request and receive a refreshable braille display from the National Library Service for the Blind and Print Disabled (NLS).\nSome important things to note: * You must be an NLS patron. * Availability of the refreshable braille display eReaders from the NLS is not guaranteed. * While there is no cost associated, patrons are loaning these eReaders from the NLS for an unspecified period of time."
  },
  {
    "objectID": "braille-display.html#how-to-request-an-nls-refreshable-braille-display-ereader",
    "href": "braille-display.html#how-to-request-an-nls-refreshable-braille-display-ereader",
    "title": "How to Request an NLS eReader Braille Display?",
    "section": "How to Request an NLS Refreshable Braille Display (eReader)",
    "text": "How to Request an NLS Refreshable Braille Display (eReader)\n\nIf you are not one already, enroll to become an NLS patron. Be sure to refer to the eligibility criteria and steps outlined on that page before you apply.\nOnce you are enrolled by the NLS as a patron and have access to their services, find your network library’s contact information from this page.\nOnce you have identified your regional library, give them a call and ask them if you can request an NLS refreshable braille display eReader to read books from BARD.\nIf they are still accepting requests for these eReaders, they’ll take down your name, mailing address, and any other relevant information. Please ensure that you provide them with a mailing address that you feel comfortable having the braille display sent to.\nIt will take anywhere between 1 to 4 weeks for them to process your request and mail you the loaner eReader. This estimate may have changed since we went through the process in Q2 of 2024.\nOnce you receive the device, feel free to reference the enclosed materials like the User Guide. You’ll want to connect the device to your computer to meet our eligibility criterion for some of our ongoing studies.\n\nFor more information about this process, or to learn about available NLS equipment and other device recommendations from the NLSk, check out this Equipment for NLS Materials page.\nIf you have any questions or concerns, please email xability-lab@illinois.edu, and include NLS, all caps in the subject line.\nWe thank you for your expressed interest in our research and look forward to your participation in our ongoing studies.\nThis guide was last updated on September 24, 2024 at 9 PM CDT"
  }
]