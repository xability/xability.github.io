
@inproceedings{lee2021collabally,
  title     = {CollabAlly: Accessible Collaboration Awareness in Document Editing},
  author    = {Lee, Cheuk Yin Phipson and Zhang, Zhuohao and Herskovitz, Jaylin and Seo, JooYoung and Guo, Anhong},
  booktitle = {The 23rd International ACM SIGACCESS Conference on Computers and Accessibility},
  month     = {10},
  number    = {55},
  doi       = {10.1145/3441852.3476562},
  pages     = {1--4},
  year      = {2021},
}

@inproceedings{leeCollabAllyAccessibleCollaboration2022a,
  title      = {{{CollabAlly}}: {{Accessible Collaboration Awareness}} in {{Document Editing}}},
  shorttitle = {{{CollabAlly}}},
  booktitle  = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author     = {Lee, Cheuk Yin Phipson and Zhang, Zhuohao and Herskovitz, Jaylin and Seo, JooYoung and Guo, Anhong},
  date       = {2022-04-29},
  series     = {{{CHI}} '22},
  pages      = {1--17},
  publisher  = {{Association for Computing Machinery}},
  location   = {{New York, NY, USA}},
  doi        = {10.1145/3491102.3517635},
  url        = {https://doi.org/10.1145/3491102.3517635},
  urldate    = {2022-04-30},
  abstract   = {Collaborative document editing tools are widely used in professional and academic workplaces. While these tools provide basic accessibility support, it is challenging for blind users to gain collaboration awareness that sighted people can easily obtain using visual cues (e.g., who is editing where and what). Through a series of co-design sessions with a blind coauthor, we identified the current practices and challenges in collaborative editing, and iteratively designed CollabAlly, a system that makes collaboration awareness in document editing accessible to blind users. CollabAlly extracts collaborator, comment, and text-change information and their context from a document and presents them in a dialog box to provide easy access and navigation. CollabAlly uses earcons to communicate background events unobtrusively, voice fonts to differentiate collaborators, and spatial audio to convey the location of document activity. In a study with 11 blind participants, we demonstrate that CollabAlly provides improved access to collaboration awareness by centralizing scattered information, sonifying visual information, and simplifying complex operations.},
  isbn       = {978-1-4503-9157-3},
  keywords   = {accessibility,Blind,collaboration awareness,Collaborative writing,earcon,screen reader,spatial audio,visual impairment,voice font},
  note       = {Honorable mention award}
}

@inproceedings{huh2022duoethnographic,
  title     = {A duoethnographic study of a mixed-ability team in a collaborative group programming project},
  author    = {Mina Huh and JooYoung Seo},
  year      = {2022},
  pages     = {471-474},
  editor    = {Armin Weinberger and Wenli Chen and Davinia Hernández-Leo and Bodong Chen},
  booktitle = {Proceedings of the 15th International Conference on Computer-Supported Collaborative Learning - CSCL 2022},
  publisher = {International Society of the Learning Sciences},
}

@inproceedings{teachingvisualaccessibility2022,
  title     = {Teaching Visual Accessibility in the Introductory Data Science Classes: Why, What, When, and How},
  booktitle = {the 2022 Symposium on Data Science & Statistics},
  author    = {Seo, JooYoung and Dogucu, Mine},
  year      = {2022},
  publisher = {American Statistical Association},
  location  = {{Pittsburgh, PA, USA}},
}

@inproceedings{289508,
  author    = {Kaushik, Smirity and Barbosa, Natã M. and Yu, Yaman and
               Sharma, Tanusree and Kilhoffer, Zachary and Seo, JooYoung and Das,
               Sauvik and Wang, Yang},
  publisher = {USENIX Association},
  title     = {{{GuardLens}}: {Supporting} Safer Online Browsing for People
               with Visual Impairments},
  booktitle = {Nineteenth symposium on usable privacy and security
               (SOUPS 2023)},
  pages     = {361-380},
  date      = {2023-08},
  address   = {Anaheim, CA},
  url       = {https://www.usenix.org/conference/soups2023/presentation/kaushik},
  isbn      = {978-1-939133-36-6},
}

@inproceedings{289496,
  author    = {Zhang, Zhuohao (Jerry) and Kaushik, Smirity and Seo,
               JooYoung and Yuan, Haolin and Das, Sauvik and Findlater, Leah and
               Gurari, Danna and Stangl, Abigale and Wang, Yang},
  publisher = {USENIX Association},
  title     = {{{ImageAlly}}: {A} {{Human-AI}} Hybrid Approach to Support
               Blind People in Detecting and Redacting Private Image Content},
  booktitle = {Nineteenth symposium on usable privacy and security
               (SOUPS 2023)},
  pages     = {417-436},
  date      = {2023-08},
  address   = {Anaheim, CA},
  url       = {https://www.usenix.org/conference/soups2023/presentation/zhang},
  isbn      = {978-1-939133-36-6},
}


@article{kimTrendCollaborationSTEM2023a,
  title        = {Trend of {{Collaboration}} in {{STEM Education}} in {{Informal Learning Institutions Based}} on {{IMLS-funded Projects}}},
  author       = {Kim, Soo Hyeon and Yoon, Ayoung and Seo, JooYoung},
  date         = {2023},
  journaltitle = {Proceedings of the Association for Information Science and Technology},
  volume       = {60},
  number       = {1},
  pages        = {625--629},
  issn         = {2373-9231},
  doi          = {10.1002/pra2.828},
  url          = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pra2.828},
  urldate      = {2023-10-26},
  abstract     = {There is limited systematic research on understanding the trends of STEM education in libraries. While reviews of STEM education from various funding agencies exist, these reviews encompass STEM education across formal and informal settings, which may not provide specific implications that center around libraries. This paper examines the trend of collaboration among libraries and other collaborating organizations involved in STEM education funded by the Institute of Museum and Library Services (IMLS). Through content analysis and geocode analysis of 128 projects that are funded by IMLS in 2012–2022, this study shows the diverse engagement of different types of informal learning institutions in STEM education over time. Findings also demonstrate that while few informal learning institutions represented the leading and collaborating organizations in IMLS STEM education initially, leading and collaborating organizations diversified with a higher level of collaboration.},
  langid       = {english},
  keywords     = {Collaboration,Geocode analysis,IMLS funding,STEM education,Trend},
}


@article{parkExploringOnlineCommunity2023,
  title        = {Exploring an {{Online Community}} of {{Blind Programmers}} by {{Using Topic Modeling}} and {{Network Analysis}}},
  author       = {Park, Jaihyun and Seo, JooYoung and Lee, Jae Young},
  date         = {2023},
  journaltitle = {Proceedings of the Association for Information Science and Technology},
  volume       = {60},
  number       = {1},
  pages        = {1096--1098},
  issn         = {2373-9231},
  doi          = {10.1002/pra2.956},
  url          = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pra2.956},
  urldate      = {2023-10-26},
  abstract     = {Much work has been carried out to highlight the accessibility challenges of blind programmers. Yet, relatively little has been known about how blind programmers help each other to solve problems. We present a data-driven approach to explore collaborative problem-solving of users in the Program-l community of, by, and for blind programmers. We collected 8,344 longitudinal email threads from 778 users from 2004 through 2022 to observe the dynamics of collaborative problem-solving among blind programmers. Our embedding-based topic modeling and assortativity network analysis reveal that the knowledge of blind programmers diverges between when asking and answering questions. Our findings also suggest that users who have a high cluster level in the first year of activity and members are more likely to interact with other members with different roles. Our paper contributes to the field of social computing by introducing the first large-scale study of a unique community of blind programmers.},
  langid       = {english},
  keywords     = {Blind programmers,Collaborative problem-solving,Online community,Social computing},
}

@inproceedings{seoCodingNonVisuallyVisual2023,
  title      = {Coding {{Non-Visually}} in {{Visual Studio Code}}: {{Collaboration Towards Accessible Development Environment}} for {{Blind Programmers}}},
  shorttitle = {Coding {{Non-Visually}} in {{Visual Studio Code}}},
  booktitle  = {Proceedings of the 25th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author     = {Seo, JooYoung and Rogge, Megan},
  date       = {2023-10-22},
  series     = {{{ASSETS}} '23},
  pages      = {1--9},
  publisher  = {{Association for Computing Machinery}},
  location   = {{New York, NY, USA}},
  doi        = {10.1145/3597638.3614550},
  url        = {https://dl.acm.org/doi/10.1145/3597638.3614550},
  urldate    = {2023-10-26},
  abstract   = {This paper delineates a fruitful collaboration between blind and sighted developers, aiming to augment the accessibility of Visual Studio Code (VSCode). Our shared journey is portrayed through examples drawn from our interaction with GitHub issues, pull requests, review processes, and insider’s releases, each contributing to an improved VSCode experience for blind developers. One key milestone of our co-design process is the establishment of an accessible terminal buffer, a significant enhancement for blind developers using VSCode. Other innovative outcomes include Git Diff audio cues, adaptable verbosity settings, intuitive help menus, and a targeted accessibility testing initiative. These tailored improvements not only uplift the accessibility standards of VSCode but also provide a valuable blueprint for open-source developers at large. Through our shared dedication to promoting inclusivity in software development, we aim for the strategies and successes shared in this paper to inspire and guide the open-source community towards crafting more accessible software environments.},
  isbn       = {9798400702204},
  keywords   = {accessibility,integrated development environment,nonvisual programming,visual studio code},
}

@inproceedings{seoMAIDRMultimodalAccess2023,
  title     = {{{MAIDR}}: {{Multimodal Access}} and {{Interactive Data Representation System}} for {{Inclusive Data Science Education}}},
  booktitle = {Proceedings of the 3rd {{Annual Meeting}} of the {{International Society}} of the {{Learning Sciences}}},
  author    = {Seo, JooYoung and Xia, Yilin and Yam, Yu Jun and McCurry, Sean},
  date      = {2023},
  pages     = {51--54},
}


@inproceedings{seoMAIDRMakingStatistical2024,
  title      = {{{MAIDR}}: {{Making Statistical Visualizations Accessible}} with {{Multimodal Data Representation}}},
  shorttitle = {{{MAIDR}}},
  booktitle  = {Proceedings of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author     = {Seo, JooYoung and Xia, Yilin and Lee, Bongshin and Mccurry, Sean and Yam, Yu Jun},
  date       = {2024-05-11},
  series     = {{{CHI}} '24},
  pages      = {1--22},
  publisher  = {Association for Computing Machinery},
  location   = {New York, NY, USA},
  doi        = {10.1145/3613904.3642730},
  url        = {https://dl.acm.org/doi/10.1145/3613904.3642730},
  urldate    = {2024-05-14},
  abstract   = {This paper investigates new data exploration experiences that enable blind users to interact with statistical data visualizations—bar plots, heat maps, box plots, and scatter plots—leveraging multimodal data representations. In addition to sonification and textual descriptions that are commonly employed by existing accessible visualizations, our MAIDR (multimodal access and interactive data representation) system incorporates two additional modalities (braille and review) that offer complementary benefits. It also provides blind users with the autonomy and control to interactively access and understand data visualizations. In a user study involving 11 blind participants, we found the MAIDR system facilitated the accurate interpretation of statistical visualizations. Participants exhibited a range of strategies in combining multiple modalities, influenced by their past interactions and experiences with data visualizations. This work accentuates the overlooked potential of combining refreshable tactile representation with other modalities and elevates the discussion on the importance of user autonomy when designing accessible data visualizations.},
  isbn       = {9798400703300},
  keywords   = {Accessibility,Blind,Braille Display,Multimodality,Screen Readers,Statistical Visualization},
}

        
@inproceedings{10.2312:eved.20241053,
  booktitle = {EuroVis 2024 - Education Papers},
  editor    = {Firat, Elif E. and Laramee, Robert S. and Andersen, Nicklas Sindelv},
  title     = {{Designing Born-Accessible Courses in Data Science and Visualization: Challenges and Opportunities of a Remote Curriculum Taught by Blind Instructors
               to Blind Students}},
  author    = {JooYoung Seo and Sile O'Modhrain and Yilin Xia and Sanchita Kamath and Bongshin Lee and James M. Coughlan},
  year      = {2024},
  publisher = {The Eurographics Association},
  isbn      = {978-3-03868-257-8},
  doi       = {10.2312/eved.20241053},
}


@article{leeIdentifyAdaptPersist2024,
  title        = {Identify, {{Adapt}}, {{Persist}}: {{The Journey}} of {{Blind Individuals}} with {{Personal Health Technologies}}},
  shorttitle   = {Identify, {{Adapt}}, {{Persist}}},
  author       = {Lee, Jarrett G.W. and Lee, Bongshin and Choi, Soyoung and Seo, JooYoung and Choe, Eun Kyoung},
  date         = {2024-05-15},
  journaltitle = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  shortjournal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  volume       = {8},
  number       = {2},
  pages        = {51:1--51:21},
  doi          = {10.1145/3659585},
  url          = {https://dl.acm.org/doi/10.1145/3659585},
  urldate      = {2024-05-16},
  abstract     = {Personal health technologies (PHTs) often do not consider the accessibility needs of blind individuals, preventing access to their capabilities and data. However, despite the accessibility barriers, some blind individuals persistently use such systems and even express satisfaction with them. To obtain a deeper understanding of blind users' prolonged experiences in PHTs, we interviewed 11 individuals who continue to use such technologies, discussing and observing their past and current interactions with their systems. We report on usability issues blind users encounter and how they adapt to these situations, and theories for the persistent use of PHTs in the face of poor accessibility. We reflect on strategies to improve the accessibility and usability of PHTs for blind users, as well as ideas to aid the normalization of accessible features within these systems.},
  keywords     = {Accessibility,Blindness,Low Vision,mHealth,Personal Health Data,Personal Health Technologies,Screen Reader,Vision Impairment},
}

@inproceedings{chenRoleVerbalDescriptions2024,
  author    = {Chen, Si and Cox, Eugene and Koh, Kyungwon and Seo, JooYoung},
  title     = {The Role of Verbal Descriptions in Facilitating Computational Thinking Skills for {BVI} Learners},
  booktitle = {Proceedings of the 18th International Conference of the Learning Sciences - {ICLS} 2024},
  editor    = {Lindgren, R. and Asino, T. I. and Kyza, E. A. and Looi, C. K. and Keifert, D. T. and Suárez, E.},
  year      = {2024},
  pages     = {2343--2344},
  publisher = {International Society of the Learning Sciences},
  doi       = {10.22318/icls2024.594746},
}

@inproceedings{jungLearningEnvironmentsDesigned2024,
  author    = {Jung, Yong Ju and Chang, Yunjeong and Moon, Jewoong and Seo, JooYoung and Bonnette, Rachel and Lee, Joo-Young and Ke, Fengfeng and Sokolikj, Zlatko and Koh, Kyungwon and Cox, Eugene and Chen, Si and Abbas, June and Munyao, Moses and DiCioccio, Mary and Alstad, Zac},
  title     = {Learning Environments Designed For and With Learners with Disabilities},
  booktitle = {Proceedings of the 18th International Conference of the Learning Sciences - {ICLS} 2024},
  editor    = {Lindgren, R. and Asino, T. I. and Kyza, E. A. and Looi, C. K. and Keifert, D. T. and Suárez, E.},
  year      = {2024},
  pages     = {1918--1925},
  publisher = {International Society of the Learning Sciences},
  doi       = {10.22318/icls2024.384050},
}

@inproceedings{seoMAIDRAI2024,
  title     = {{MAIDR Meets AI: Exploring Multimodal LLM-Based Data Visualization Interpretation by and with Blind and Low-Vision Users}},
  booktitle = {Proceedings of the 26th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author    = {JooYoung Seo and Sanchita S. Kamath and Aziz Zeidieh and Saairam Venkatesh and Sean McCurry},
  date      = {2024-10-28},
  series    = {{{ASSETS}} '24},
  publisher = {{Association for Computing Machinery}},
  location  = {{St. John's, NL, Canada}},
  doi       = {10.1145/3663548.3675660},
  url       = {https://dl.acm.org/doi/10.1145/3663548.3675660},
  urldate   = {2024-10-28},
  isbn      = {979-8-4007-0677-6/24/10},
}


@inproceedings{kamathPlayingVR2024,
  title     = {{Playing Without Barriers: Crafting Playful and Accessible VR Table-Tennis with and for Blind and Low-Vision Individuals}},
  booktitle = {Proceedings of the 26th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author    = {Sanchita S. Kamath and Aziz Zeidieh and Omar Khan and Dhruv Sethi and JooYoung Seo},
  date      = {2024-10-28},
  series    = {{{ASSETS}} '24},
  publisher = {{Association for Computing Machinery}},
  location  = {{St. John's, NL, Canada}},
  doi       = {10.1145/3663548.3675660},
  url       = {https://dl.acm.org/doi/10.1145/3663548.3675660},
  urldate   = {2024-10-28},
  isbn      = {979-8-4007-0677-6/24/10},
}


@inproceedings{geStereoMath2024,
  title     = {{StereoMath: An Accessible and Musical Equation Editor}},
  booktitle = {Proceedings of the 26th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author    = {Kenneth Ge and JooYoung Seo},
  date      = {2024-10-28},
  series    = {{{ASSETS}} '24},
  publisher = {{Association for Computing Machinery}},
  location  = {{St. John's, NL, Canada}},
  doi       = {10.1145/3663548.3688487},
  url       = {https://dl.acm.org/doi/10.1145/3663548.3688487},
  urldate   = {2024-10-28},
  isbn      = {979-8-4007-0677-6/24/10},
}


@inproceedings{zhouTeachingDifferent2024,
  title     = {{Teaching Accessibility in Different Disciplines: Topics, Approaches, Resources, Challenges}},
  booktitle = {Proceedings of the 26th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author    = {Kyrie Zhixuan Zhou and Rachel F. Adler and Caterina Almendral and Soyoung Choi and Devorah Kletenik and Bruno Oro and JooYoung Seo},
  date      = {2024-10-28},
  series    = {{{ASSETS}} '24},
  publisher = {{Association for Computing Machinery}},
  location  = {{St. John's, NL, Canada}},
  doi       = {10.1145/3663548.3688553},
  url       = {https://dl.acm.org/10.1145/3663548.3688553},
  urldate   = {2024-10-28},
  isbn      = {979-8-4007-0677-6/24/10},
}

@article{leePersonalHealthData2023,
  title        = {Personal {{Health Data Tracking}} by {{Blind}} and {{Low-Vision People}}: {{Survey Study}}},
  shorttitle   = {Personal {{Health Data Tracking}} by {{Blind}} and {{Low-Vision People}}},
  author       = {Lee, Jarrett G. W. and Lee, Kyungyeon and Lee, Bongshin and Choi, Soyoung and Seo, JooYoung and Choe, Eun Kyoung},
  date         = {2023-05-04},
  journaltitle = {Journal of Medical Internet Research},
  volume       = {25},
  number       = {1},
  pages        = {e43917},
  publisher    = {{JMIR Publications Inc., Toronto, Canada}},
  doi          = {10.2196/43917},
  url          = {https://www.jmir.org/2023/1/e43917},
  urldate      = {2023-05-10},
  abstract     = {Background: Personal health technologies, including wearable tracking devices and mobile apps, have great potential to equip the general population with the ability to monitor and manage their health. However, being designed for sighted people, much of their functionality is largely inaccessible to the blind and low-vision (BLV) population, threatening the equitable access to personal health data (PHD) and health care services. Objective: This study aims to understand why and how BLV people collect and use their PHD and the obstacles they face in doing so. Such knowledge can inform accessibility researchers and technology companies of the unique self-tracking needs and accessibility challenges that BLV people experience. Methods: We conducted a web-based and phone survey with 156 BLV people. We reported on quantitative and qualitative findings regarding their PHD tracking practices, needs, accessibility barriers, and work-arounds. Results: BLV respondents had strong desires and needs to track PHD, and many of them were already tracking their data despite many hurdles. Popular tracking items (ie, exercise, weight, sleep, and food) and the reasons for tracking were similar to those of sighted people. BLV people, however, face many accessibility challenges throughout all phases of self-tracking, from identifying tracking tools to reviewing data. The main barriers our respondents experienced included suboptimal tracking experiences and insufficient benefits against the extended burden for BLV people. Conclusions: We reported the findings that contribute to an in-depth understanding of BLV people’s motivations for PHD tracking, tracking practices, challenges, and work-arounds. Our findings suggest that various accessibility challenges hinder BLV individuals from effectively gaining the benefits of self-tracking technologies. On the basis of the findings, we discussed design opportunities and research areas to focus on making PHD tracking technologies accessible for all, including BLV people. Trial Registration:},
  langid       = {english},
}

@article{choiConversationalAgentsMHealth2024,
  title        = {Conversational Agents in {{mHealth}}: Use Patterns, Challenges, and Design Opportunities for Individuals with Visual Impairments},
  shorttitle   = {Conversational Agents in {{mHealth}}},
  author       = {Choi, Soyoung and Seo, JooYoung and Hernandez, Manuel and Kitsiou, Spyros},
  date         = {2024-03-26},
  journaltitle = {Journal of Technology in Behavioral Science},
  shortjournal = {J. technol. behav. sci.},
  issn         = {2366-5963},
  doi          = {10.1007/s41347-024-00409-7},
  url          = {https://doi.org/10.1007/s41347-024-00409-7},
  urldate      = {2024-05-07},
  abstract     = {Individuals with visual impairments often encounter significant accessibility challenges in using mobile health (mHealth) technologies such as apps and wearable sensor devices. In this context, conversational agents (CAs) hold potential as transformative tools for facilitating their interaction with mHealth. However, no prior research has investigated their needs and challenges in mHealth’s conversational agents. To address this gap, this study investigated the use patterns, challenges, and design opportunities for individuals with visual impairments.},
  langid       = {english},
  keywords     = {Accessibility,Conversational agents,Mobile health,Visual impairments,Voice user interface},
}

@article{seoTeachingVisualAccessibility2023,
  title        = {Teaching {{Visual Accessibility}} in {{Introductory Data Science Classes}} with {{Multi-Modal Data Representations}}},
  author       = {Seo, JooYoung and Dogucu, Mine},
  date         = {2023-03-21},
  journaltitle = {Journal of Data Science},
  pages        = {1--14},
  publisher    = {{School of Statistics, Renmin University of China}},
  issn         = {1680-743X, 1683-8602},
  doi          = {10.6339/23-JDS1095},
  url          = {https://jds-online.org/journal/JDS/article/1331},
  urldate      = {2023-04-21},
  abstract     = {Although there are various ways to represent data patterns and models, visualization has been primarily taught in many data science courses for its efficiency. Such vision-dependent output may cause critical barriers against those who are blind and visually impaired and people with learning disabilities. We argue that instructors need to teach multiple data representation methods so that all students can produce data products that are more accessible. In this paper, we argue that accessibility should be taught as early as the introductory course as part of the data science curriculum so that regardless of whether learners major in data science or not, they can have foundational exposure to accessibility. As data science educators who teach accessibility as part of our lower-division courses in two different institutions, we share specific examples that can be utilized by other data science instructors.},
  langid       = {english},
}


@article{moonRevisitingMultimediaLearning2023,
  title        = {Revisiting Multimedia Learning Design Principles in Virtual Reality-Based Learning Environments for Autistic Individuals},
  author       = {Moon, Jewoong and Choi, Gi Woong and Seo, Joo Young},
  date         = {2023-09-09},
  journaltitle = {Virtual Reality},
  shortjournal = {Virtual Reality},
  issn         = {1434-9957},
  doi          = {10.1007/s10055-023-00856-2},
  url          = {https://doi.org/10.1007/s10055-023-00856-2},
  urldate      = {2023-09-10},
  abstract     = {Virtual reality (VR) offers promising opportunities for supporting autistic learners in developing social and cognitive skills. However, designing VR-based learning environments optimized for these learners requires a nuanced understanding that addresses their unique needs. This review article reconsiders the key theories of multimedia learning, including cognitive load theory and cognitive theory of multimedia learning, with the aim of illuminating how these theories can inform the development of effective VR-based learning environments for autistic learners. We propose four design goals for a VR-based learning environment optimized for autistic learners: (1) minimizing learners’ extraneous load via attention guiding, (2) managing intrinsic cognitive load in problem-solving, (3) fostering germane processing through multiple representations, and (4) assessing cognitive load and implementing adaptive learning support design. In this exploration, we bring to demonstrate prevalent design challenges of existing VR-based learning environments for autistic individuals and offer prospective research trajectories for their enhancement. By incorporating a strengths-based approach, accommodating the diverse sensory needs, and recognizing the cognitive differences among autistic individuals, we aspire to advance a more inclusive VR design practice. This review presents crucial insights and direction for researchers and designers aiming to create effective, accessible, and inclusive VR-based learning environments for autistic learners and beyond.},
  langid       = {english},
  keywords     = {Autism,Cognitive load,Multimedia learning design,Virtual reality},
}
